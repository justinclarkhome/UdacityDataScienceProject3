{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime as dt\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "idx = pd.IndexSlice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your Tasks\n",
    "Your project will be divided into the following tasks\n",
    "\n",
    "I. Exploratory Data Analysis\n",
    "\n",
    "Before making recommendations of any kind, you will need to explore the data you are working with for the project. Dive in to see what you can find. There are some basic, required questions to be answered about the data you are working with throughout the rest of the notebook. Use this space to explore, before you dive into the details of your recommendation system in the later sections.\n",
    "\n",
    "II. Rank Based Recommendations\n",
    "\n",
    "To get started in building recommendations, you will first find the most popular articles simply based on the most interactions. Since there are no ratings for any of the articles, it is easy to assume the articles with the most interactions are the most popular. These are then the articles we might recommend to new users (or anyone depending on what we know about them).\n",
    "\n",
    "III. User-User Based Collaborative Filtering\n",
    "\n",
    "In order to build better recommendations for the users of IBM's platform, we could look at users that are similar in terms of the items they have interacted with. These items could then be recommended to the similar users. This would be a step in the right direction towards more personal recommendations for the users. You will implement this next.\n",
    "\n",
    "IV. Content Based Recommendations (EXTRA - NOT REQUIRED)\n",
    "\n",
    "Given the amount of content available for each article, there are a number of different ways in which someone might choose to implement a content based recommendations system. Using your NLP skills, you might come up with some extremely creative ways to develop a content based recommendation system. You are encouraged to complete a content based recommendation system, but not required to do so to complete this project.\n",
    "\n",
    "V. Matrix Factorization\n",
    "\n",
    "Finally, you will complete a machine learning approach to building recommendations. Using the user-item interactions, you will build out a matrix decomposition. Using your decomposition, you will get an idea of how well you can predict new articles an individual might interact with (spoiler alert - it isn't great). You will finally discuss which methods you might use moving forward, and how you might test how well your recommendations are working for engaging users.\n",
    "\n",
    "\n",
    "Before you submit your work, check the [RUBRIC](https://learn.udacity.com/nanodegrees/nd025/parts/cd0019/lessons/ls11961/concepts/ls11961-project-rubric) (opens in a new tab)to make sure you meet all of the rubric items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Things to remember\n",
    "- [Course 5, chapter 3.6](https://learn.udacity.com/nanodegrees/nd025/parts/cd0019/lessons/27c53b6f-1da3-42e0-805b-51dfee1c61b8/concepts/20d75ef1-8718-4441-bb6b-70603fb8f1c0)\n",
    "- [Course 5, chapter 3.3](https://learn.udacity.com/nanodegrees/nd025/parts/cd0019/lessons/27c53b6f-1da3-42e0-805b-51dfee1c61b8/concepts/5f35a1d2-0a02-42c0-949d-490a0abc9e17?lesson_tab=lesson)\n",
    "- [Coutse 5, chapter 7.11](https://learn.udacity.com/nanodegrees/nd025/parts/cd0019/lessons/ls11960/concepts/3824dbbd-f9b2-42e4-9479-046e3bdee1a5?lesson_tab=lesson)\n",
    "\n",
    "For a binomial distribution, the standard error given some probability of success p and sample size n is:\n",
    "$$\n",
    "SE = \\sqrt{\\frac{p(1-p)}{n}}\n",
    "$$\n",
    "\n",
    "When calculating statistical power, the standard error of the null is then:\n",
    "$$\n",
    "SE_{null} = \\sqrt{\\frac{p(1-p) + p(1-p)}{n}}\n",
    "$$\n",
    "\n",
    "For SVD:\n",
    "\n",
    "$$\n",
    "M = U \\sigma V^T\n",
    "$$\n",
    "\n",
    "$$ U_{n x k} $$\n",
    "$$\\Sigma_{k x k} $$\n",
    "$$V^T_{k x m} $$\n",
    "\n",
    "where:\n",
    "\n",
    "1. n is the number of users\n",
    "2. k is the number of latent features to keep (4 for this case) - there can be as many latent features are there are items (like PCA). So in the movie example, with users along the index and movie IDs along columns, this would be, at most, the number of movies.\n",
    "3. m is the number of movies\n",
    "\n",
    "Notes:\n",
    "\n",
    "1. $U$ is a square matrix with the number of rows and columns equaling the number of users. \n",
    "2. $V^T$ is also a square matrix with the number of rows and columns equaling the number of items.\n",
    "3. $\\sigma$ is actually returned as just an array with 4 values.  \n",
    "\n",
    "In order to set up the matrices in a way that they can be multiplied together, we have a few steps to perform: \n",
    "\n",
    "1. Turn sigma into a square matrix with the number of latent features we would like to keep. \n",
    "2. Change the columns of u and the rows of v transpose to match this number of dimensions. \n",
    "If we would like to exactly re-create the user-movie matrix, we could choose to keep all of the latent features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistical_power(p_null, p_alt, n, alpha = .05, plot = True):\n",
    "    \"\"\"\n",
    "    Compute the power of detecting the difference in two populations with \n",
    "    different proportion parameters, given a desired alpha rate.\n",
    "    \n",
    "    Input parameters:\n",
    "        p_null: base success rate under null hypothesis\n",
    "        p_alt : desired success rate to be detected, must be larger than\n",
    "                p_null\n",
    "        n     : number of observations made in each group\n",
    "        alpha : Type-I error rate\n",
    "        plot  : boolean for whether or not a plot of distributions will be\n",
    "                created\n",
    "    \n",
    "    Output value:\n",
    "        power : Power to detect the desired difference, under the null.\n",
    "    \"\"\"\n",
    "\n",
    "    # The example explored is a one-tailed test, with the alternative value greater than the null. \n",
    "    # The power computations performed in the first part will _not_ work if the alternative proportion \n",
    "    # is greater than the null, e.g. detecting a proportion parameter of 0.88 against a null of 0.9. \n",
    "    # You might want to try to rewrite the code to handle that case! The same issue should not show \n",
    "    # up for the second approach, where we directly compute the sample size.    \n",
    "    \n",
    "    # assert p_null > p_alt, \"Cannot handle situaiton where p_alt is less than p_null\"\n",
    "\n",
    "    # Compute the power\n",
    "    se_null = np.sqrt((p_null * (1-p_null) + p_null * (1-p_null)) / n)\n",
    "    null_dist = stats.norm(loc = 0, scale = se_null)\n",
    "    p_crit = null_dist.ppf(1 - alpha)\n",
    "    \n",
    "    se_alt  = np.sqrt((p_null * (1-p_null) + p_alt  * (1-p_alt) ) / n)\n",
    "    alt_dist = stats.norm(loc = p_alt - p_null, scale = se_alt)\n",
    "    beta = alt_dist.cdf(p_crit)\n",
    "    \n",
    "    if plot:\n",
    "        # Compute distribution heights\n",
    "        low_bound = null_dist.ppf(.01)\n",
    "        high_bound = alt_dist.ppf(.99)\n",
    "        x = np.linspace(low_bound, high_bound, 201)\n",
    "        y_null = null_dist.pdf(x)\n",
    "        y_alt = alt_dist.pdf(x)\n",
    "\n",
    "        # Plot the distributions\n",
    "        plt.plot(x, y_null)\n",
    "        plt.plot(x, y_alt)\n",
    "        plt.vlines(p_crit, 0, np.amax([null_dist.pdf(p_crit), alt_dist.pdf(p_crit)]),\n",
    "                   linestyles = '--')\n",
    "        plt.fill_between(x, y_null, 0, where = (x >= p_crit), alpha = .5)\n",
    "        plt.fill_between(x, y_alt , 0, where = (x <= p_crit), alpha = .5)\n",
    "        \n",
    "        plt.legend(['null','alt'])\n",
    "        plt.xlabel('difference')\n",
    "        plt.ylabel('density')\n",
    "        plt.show()\n",
    "    \n",
    "    # return power\n",
    "    return (1 - beta)\n",
    "\n",
    "def experiment_size(p_null, p_alt, alpha = .05, beta = .20):\n",
    "    \"\"\"\n",
    "    Compute the minimum number of samples needed to achieve a desired power\n",
    "    level for a given effect size.\n",
    "    \n",
    "    Input parameters:\n",
    "        p_null: base success rate under null hypothesis\n",
    "        p_alt : desired success rate to be detected\n",
    "        alpha : Type-I error rate\n",
    "        beta  : Type-II error rate\n",
    "    \n",
    "    Output value:\n",
    "        n : Number of samples required for each group to obtain desired power\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get necessary z-scores and standard deviations (@ 1 obs per group)\n",
    "    z_null = stats.norm.ppf(1 - alpha)\n",
    "    z_alt  = stats.norm.ppf(beta)\n",
    "    sd_null = np.sqrt(p_null * (1-p_null) + p_null * (1-p_null))\n",
    "    sd_alt  = np.sqrt(p_null * (1-p_null) + p_alt  * (1-p_alt) )\n",
    "    \n",
    "    # Compute and return minimum sample size\n",
    "    p_diff = p_alt - p_null\n",
    "    n = ((z_null*sd_null - z_alt*sd_alt) / p_diff) ** 2\n",
    "    return np.ceil(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistical_power(\n",
    "    p_null=0.15, p_alt=0.175, n=520, alpha = .05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_size(.1, .12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical power - really not sure how they arrice at their specific values, especially the \"rate\" decimals.\n",
    "\n",
    "\n",
    "\n",
    "alpha = 0.05\n",
    "power = 0.8\n",
    "Z_alphadiv2 = stats.norm.ppf(1-(alpha/2))\n",
    "Z_beta = stats.norm.ppf(power)\n",
    "\n",
    "start_days = 520\n",
    "p0 = 0.15\n",
    "p1 = 0.175\n",
    "\n",
    "num_term1 = np.power(Z_alphadiv2 + Z_beta, 2)\n",
    "num_term2 = p0 * (1-p0) + p1 * (1-p1)\n",
    "num = num_term1 * num_term2\n",
    "den = np.power(p1-p0, 2) * start_days\n",
    "\n",
    "n = num/den\n",
    "\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CRISP-DM Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
